# API Testing Agent ğŸ¤–

> **Automated API test generation and execution powered by LLM**

An intelligent orchestration service that automatically:
- ğŸ“‹ Reads Jira Epic specifications
- ğŸ” Fetches and parses OpenAPI specifications
- ğŸ§ª Generates comprehensive API tests using LLM (OpenAI GPT)
- ğŸ“¦ Commits tests to GitHub on feature branches
- ğŸ³ Runs tests in isolated Docker containers
- ğŸ“Š Reports results back to Jira

---

## âœ¨ Key Features

- **ğŸ¯ Smart Contract Extraction**: Automatically fetches OpenAPI specs, parses endpoint definitions, or infers from descriptions
- **ğŸ¤– LLM-Powered Generation**: Uses GPT-4 to generate comprehensive pytest tests with full coverage
- **ğŸ”„ Full Automation**: Webhook â†’ Fetch â†’ Generate â†’ Commit â†’ Test â†’ Report
- **ğŸŒ Multiple Input Formats**: Supports OpenAPI URLs, manual endpoints, or plain descriptions
- **ğŸ“ ADF Support**: Handles Jira API v3's Atlassian Document Format
- **ğŸ³ Isolated Execution**: Runs tests in Docker containers for safety
- **âœ… Comprehensive Testing**: Includes happy path, validation, 404s, auth, and edge cases

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Jira Epic   â”‚  â† Contains OpenAPI URL or endpoint definitions
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Webhook
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Webhook    â”‚             â”‚  Polling Runner          â”‚
â”‚  (core/webhooks.py) â”‚             â”‚  (core/epic_runner.py)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORCHESTRATION PIPELINE                       â”‚
â”‚              (core/orchestrator.py)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Fetch Epic from Jira                                 â”‚
â”‚     â””â”€> integrations/jira/client.py                      â”‚
â”‚                                                           â”‚
â”‚  2. Extract & Fetch API Contract                         â”‚
â”‚     â””â”€> integrations/jira/contract_parser.py             â”‚
â”‚         â”œâ”€> Extract OpenAPI URL from description         â”‚
â”‚         â”œâ”€> Fetch & parse OpenAPI spec (JSON/YAML)       â”‚
â”‚         â””â”€> Extract manual endpoints or use description  â”‚
â”‚                                                           â”‚
â”‚  3. Eligibility Gate (LLM)                               â”‚
â”‚     â””â”€> services/test_generator/gating.py                â”‚
â”‚         â”œâ”€> Output: {"should_proceed": bool, "reason": ""}â”‚
â”‚         â””â”€> If false â†’ skip and post comment             â”‚
â”‚                                                           â”‚
â”‚  4. Generate Tests with LLM                              â”‚
â”‚     â””â”€> services/test_generator/generator.py             â”‚
â”‚         â”œâ”€> Format Epic + OpenAPI spec into prompt       â”‚
â”‚         â”œâ”€> Call OpenAI (configurable model)             â”‚
â”‚         â””â”€> Parse generated test files                   â”‚
â”‚                                                           â”‚
â”‚  5. Review & Refine (LLM)                                â”‚
â”‚     â””â”€> services/test_generator/reviewer.py              â”‚
â”‚         â”œâ”€> Score: syntax, coverage, criteria            â”‚
â”‚         â”œâ”€> Notes: detailed findings                     â”‚
â”‚         â””â”€> If below threshold â†’ refine with notes       â”‚
â”‚     â””â”€> services/test_generator/refiner.py               â”‚
â”‚                                                           â”‚
â”‚  6. Commit to GitHub                                     â”‚
â”‚     â””â”€> integrations/github/client.py                    â”‚
â”‚         â””â”€> Create branch: auto/tests/{epic}/{timestamp} â”‚
â”‚                                                           â”‚
â”‚  7. Run Tests in Docker                                  â”‚
â”‚     â””â”€> services/test_runner/runner.py                   â”‚
â”‚         â”œâ”€> Clone branch to temp directory               â”‚
â”‚         â”œâ”€> Run pytest in python:3.11 container          â”‚
â”‚         â””â”€> Parse JUnit XML results                      â”‚
â”‚                                                           â”‚
â”‚  8. Post Results to Jira                                 â”‚
â”‚     â””â”€> integrations/jira/client.py                      â”‚
â”‚         â””â”€> Add comment with test results                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Detailed AI Processing Flow

```mermaid
sequenceDiagram
    participant J as Jira Epic
    participant O as Orchestrator
    participant AI1 as AI-1: Gate
    participant AI2 as AI-2: Generator
    participant AI3 as AI-3: Reviewer
    participant AI4 as AI-4: Refiner
    participant GH as GitHub
    participant JK as Jenkins

    J->>O: Epic created/updated
    O->>O: Fetch & parse contract
    
    Note over O,AI1: Eligibility Check
    O->>AI1: epic + contract
    AI1-->>O: {should_proceed: bool, reason}
    
    alt should_proceed = false
        O->>J: Post gate failure reason
        Note over O: STOP
    else should_proceed = true
        Note over O,AI2: Test Generation
        O->>AI2: epic + contract + language
        AI2-->>O: files {path: content}
        
        Note over O,AI3: Review & Scoring
        O->>AI3: epic + contract + files
        AI3-->>O: {score, syntax_ok, coverage_score, criteria_score, notes}
        O->>O: Calculate avg = (coverage + criteria + syntax)/3
        
        alt avg <= threshold
            Note over O,AI4: Refinement Pass
            O->>AI4: epic + contract + files + notes
            AI4-->>O: corrected files
            O->>O: Apply corrections
        end
        
        O->>O: Write metadata.json<br/>(gate, testcases, reviewer, refiner)
        O->>O: Save locally: output/EPIC/
        
        opt GitHub enabled
            O->>GH: Push to auto/tests/EPIC/timestamp
            GH->>JK: Trigger pipeline
            JK->>JK: Run tests (pytest/mvn)
            JK->>J: Post results + branch
        end
    end
```

## ğŸ“¦ Data Flow & Artifacts

```mermaid
graph LR
    subgraph Inputs
        E[Epic Description<br/>OpenAPI URL]
        C[config.yaml]
    end

    subgraph AI_Chain["AI Processing Chain"]
        direction TB
        G[Gate Output<br/>JSON]
        T[Test Files<br/>Dict]
        R[Review Scores<br/>JSON]
        F[Refined Files<br/>Dict]
    end

    subgraph Artifacts["ğŸ“¦ Output Artifacts"]
        direction TB
        M[metadata.json<br/>â”œâ”€ gate<br/>â”œâ”€ test_cases<br/>â”œâ”€ reviewer<br/>â”œâ”€ refiner_output<br/>â””â”€ refinement_metadata]
        GI[GENERATION_INFO.txt<br/>summary]
        TF[Test Files<br/>*.py / *.java]
    end

    E --> G
    G --> T
    T --> R
    R --> F
    
    G --> M
    T --> M
    R --> M
    F --> M
    
    T --> TF
    F --> TF
    
    C -.->|threshold| R
    
    M --> GH[GitHub Branch]
    TF --> GH
    GI --> GH

    style M fill:#ffd700
    style G fill:#e1f5ff
    style T fill:#fff9e1
    style R fill:#ffe1f5
    style F fill:#e1ffe1
```

---

## ğŸ“ Project Structure

```
api_testing_agent/
â”œâ”€â”€ core/                           # Core orchestration & webhook handling
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ webhooks.py                # FastAPI app with webhook endpoints
â”‚   â””â”€â”€ orchestrator.py            # Main pipeline coordination logic
â”‚
â”œâ”€â”€ integrations/                   # External service integrations
â”‚   â”œâ”€â”€ jira/                      # Jira API client & parsing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ client.py              # Fetch issues, post comments
â”‚   â”‚   â””â”€â”€ contract_parser.py    # Extract & fetch OpenAPI specs
â”‚   â””â”€â”€ github/                    # GitHub API client
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ client.py              # Create branches, commit files
â”‚
â”œâ”€â”€ services/                       # Business logic services
â”‚   â”œâ”€â”€ test_generator/            # LLM-based test generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py           # Test generation orchestration
â”‚   â”‚   â””â”€â”€ prompts.py             # LLM prompt templates
â”‚   â””â”€â”€ test_runner/               # Test execution
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ runner.py              # Docker-based test runner
â”‚
â”œâ”€â”€ tests/                          # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_jira_integration.py   # Comprehensive integration tests
â”‚   â”œâ”€â”€ test_jira_manual.py        # Manual test script (no pytest)
â”‚   â”œâ”€â”€ README.md                  # Testing documentation
â”‚   â””â”€â”€ sample_pytest.py           # Example generated tests
â”‚
â”œâ”€â”€ config/                         # Configuration
â”œâ”€â”€ examples/                       # Example templates
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ helper.md                       # Detailed documentation
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md      # Technical implementation details
â”œâ”€â”€ notes.md                        # Architecture notes
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ env.example                    # Environment variables template
â”œâ”€â”€ pyproject.toml                 # Project metadata
â”œâ”€â”€ main.py                        # Main entry point
â””â”€â”€ docker-runner.sh               # Helper script for local testing
```

---

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.9+
- Docker (for running tests)
- Jira account with API access
- GitHub account with repository access
- OpenAI API key

### 2. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd api_testing_agent

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

Create a `.env` file in the project root:

```bash
# Copy the example
cp env.example .env

# Edit with your credentials
nano .env
```

Required environment variables (secrets only):

```bash
# Jira (secret)
JIRA_BASE=https://yourcompany.atlassian.net
JIRA_USER=your-email@example.com
JIRA_TOKEN=your_jira_api_token

# GitHub (secret)
GITHUB_TOKEN=your_github_personal_access_token

# OpenAI (secret)
OPENAI_API_KEY=sk-your-openai-api-key

# Optional Jenkins credentials (secrets)
JENKINS_USER=admin
JENKINS_TOKEN=your-jenkins-api-token
```

Non-secrets like `REPO_FULL_NAME`, `JENKINS_URL`, polling intervals, and Jira prefix now live in `config/config.yaml` (see Advanced Configuration).

### 4. Run the Service

```bash
# Option 1: Using main.py (FastAPI on 8002 by default)
python main.py --port 8002

# Start with polling runner enabled (process only epics with configured prefix)
python main.py --runner on --language python --jenkins no --port 8002

# Option 2: Using uvicorn directly
uvicorn core.webhooks:app --reload --port 8002

# The service will be available at http://localhost:8002
```

### 5. Configure Jira Webhook

1. Go to Jira â†’ Settings â†’ System â†’ WebHooks
2. Create new webhook:
   - **URL**: `http://your-server:8002/jira/webhook`
   - **Events**: Issue â†’ Updated
   - **JQL Filter** (optional): `project = YOUR_PROJECT AND issuetype = Epic`

### 6. Test the Integration

```bash
# Run the manual test script (no pytest required)
python tests/test_jira_manual.py

# Or run full test suite with pytest
pip install pytest
pytest tests/test_jira_integration.py -v
```

---

## ğŸ“ Creating a Jira Epic

### Option 1: With OpenAPI URL (Recommended â­)

```
Summary: User Management API

Description:
User Management API for CRUD operations

OpenAPI Spec: http://localhost:8000/openapi.json
# Or: https://api.example.com/swagger.yaml

This API provides comprehensive user management capabilities including
registration, authentication, profile updates, and account deletion.

Requirements:
- Proper validation on all inputs
- JWT-based authentication
- Rate limiting
- Comprehensive error handling
```

**What happens:**
1. System extracts the OpenAPI URL
2. Fetches and parses the complete specification
3. Passes full OpenAPI spec + Epic description to LLM
4. Generates comprehensive tests covering all endpoints

### Option 2: With Manual Endpoints

```
Summary: User Management API

Description:
User Management API

Endpoints:
GET /users - List all users (paginated)
POST /users - Create new user
GET /users/{id} - Get user by ID
PUT /users/{id} - Update user
DELETE /users/{id} - Delete user
POST /users/login - Authenticate user
POST /users/logout - Logout user

Each endpoint should validate inputs and return proper error codes.
```

**What happens:**
1. System extracts endpoint definitions
2. Passes endpoints + Epic description to LLM
3. Generates tests for each listed endpoint

### Option 3: With Plain Description

```
Summary: User Management API

Description:
Implement a RESTful API for user management with the following features:
- User registration with email validation
- Login/logout with JWT tokens
- Profile management (get, update)
- User deletion with soft delete
- Admin endpoints for user management
- Proper error handling and validation
```

**What happens:**
1. LLM infers reasonable endpoints from description
2. Generates appropriate tests based on requirements

---

## ğŸ§ª Generated Test Coverage

The LLM automatically generates tests for:

- âœ… **Happy Path**: Successful requests with valid data
- âŒ **Validation Errors**: Invalid inputs, missing fields, wrong types
- ğŸ” **Not Found**: Non-existent resources (404)
- ğŸ”’ **Authentication**: Unauthorized/forbidden requests (401/403)
- ğŸ”„ **Edge Cases**: Empty lists, boundary values, duplicates
- ğŸ“Š **Response Validation**: Schema validation, required fields
- ğŸ¯ **Schemathesis**: Property-based tests (if OpenAPI provided)

Example generated test structure:
```
tests/
â”œâ”€â”€ conftest.py              # Pytest fixtures & config
â”œâ”€â”€ test_users.py            # User endpoint tests
â”œâ”€â”€ test_auth.py             # Authentication tests
â”œâ”€â”€ test_schemathesis.py     # Property-based tests
â”œâ”€â”€ requirements.txt         # Test dependencies
â””â”€â”€ README.md                # How to run tests
```

---

## ğŸ”§ Advanced Configuration

### Configuration via YAML (non-secrets)

Create or edit `config/config.yaml` to control runner and defaults:

```yaml
runner:
  time_sleep: 60           # seconds between polling cycles
  prefix_jira_name: "KAN"  # only process epics from this project
  max_per_cycle: 5

github:
  repo_full_name: "owner/repo"  # token stays in env

openai:
  generation_model: "gpt-4o-mini"
  gating_model: "gpt-4o-mini"
  review_model: "gpt-4o-mini"
  review_threshold: 0.7

jenkins:
  url: "http://localhost:8080"   # non-secret; credentials remain in env
  job_name: "API-Testing-Agent"
```

Secrets remain in environment variables: `JIRA_USER`, `JIRA_TOKEN`, `GITHUB_TOKEN`, `OPENAI_API_KEY`, optional `JENKINS_USER`, `JENKINS_TOKEN`.

### Testing Locally Without Webhook

```python
# Direct invocation for testing
from core.orchestrator import process_epic

# Process a specific Epic
process_epic("EPIC-123")
```

### Using with ngrok for Local Development

```bash
# Expose local server
ngrok http 8000

# Configure Jira webhook to:
# https://your-ngrok-url.ngrok.io/jira/webhook
```

### Switching to Java Test Generation

Edit `core/orchestrator.py`:

```python
# Change this line:
generated_files = generate_tests(issue_key, epic, contract, language='python')

# To:
generated_files = generate_tests(issue_key, epic, contract, language='java')

# And use:
runner_result = run_maven_tests_in_docker(branch)
```

### Polling Runner (Jira prefix + interval)

Start the service with a background runner that polls Jira for Epics from a given project prefix and triggers the pipeline:

```bash
# Run FastAPI + start runner
python main.py --runner on --language python --jenkins no

# Or just the runner via helper script
python tests/test_orchestrator_flow.py --runner on --language python --jenkins no
```

Runner behavior:
- Sleeps `runner.time_sleep` seconds between cycles
- Processes only issues with keys starting with `runner.prefix_jira_name` (e.g., `KAN-`)
- Skips duplicates within a short-lived cache
- Uses the LLM eligibility gate before generation and posts reasons if skipped

### CLI: Direct Orchestrator Run

```bash
python tests/test_orchestrator_flow.py \
  --jira_name KAN-4 \
  --language python \
  --jenkins yes     # push to GitHub to trigger Jenkins
```

Parameters:
- `--jira_name`: Epic key to process (required when `--runner=off`)
- `--language`: `python` or `java`
- `--jenkins`: `yes` to push to GitHub (triggers Jenkins), otherwise `no`
- `--runner`: `on` to start polling runner, or `off` to process a single epic

---

## ğŸ“š Documentation

- **[helper.md](helper.md)** - Comprehensive module documentation
- **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)** - Technical implementation details
- **[notes.md](notes.md)** - Architecture decisions & production tips
- **[tests/README.md](tests/README.md)** - Testing guide

---

## ğŸ§ª Testing the Service

### Manual Test (No pytest required)

```bash
python tests/test_jira_manual.py
```

Expected output:
```
============================================================
JIRA INTEGRATION TEST
============================================================

[1/4] Checking Configuration...
âœ“ Configuration loaded

[2/4] Testing Jira Connection...
âœ“ Successfully connected to Jira
  User: Your Name

[3/4] Testing Epic Fetch: EPIC-123...
âœ“ Successfully fetched Epic
  Summary: User Management API

[4/4] Testing Contract Extraction...
âœ“ OpenAPI spec fetched successfully
  - OpenAPI version: 3.0.0
  - Total paths: 15

[BONUS] Testing OpenAPI Spec Fetching...
âœ“ Successfully fetched and parsed OpenAPI spec

âœ… ALL TESTS PASSED
```

### Full Test Suite

```bash
# Run all tests
pytest tests/ -v

# Run specific test class
pytest tests/test_jira_integration.py::TestContractExtraction -v

# With coverage
pytest tests/ --cov=. --cov-report=html
```

---

## ğŸ” Security Considerations

- **Credentials**: Use environment variables, never commit `.env`
- **Webhook Security**: Validate Jira webhook signatures in production
- **Code Execution**: Tests run in isolated Docker containers
- **API Keys**: Use secrets manager (AWS Secrets Manager, Vault) in production
- **Rate Limiting**: Implement rate limits on webhook endpoint
- **GitHub Tokens**: Use fine-grained tokens with minimal permissions

---

## ğŸš€ Production Deployment

### Recommended Improvements

1. **Task Queue**: Use Celery, RQ, or AWS SQS instead of BackgroundTasks
2. **Database**: Store Epic processing status and results
3. **Caching**: Cache OpenAPI specs to avoid repeated fetches
4. **Monitoring**: Add Datadog, New Relic, or Prometheus metrics
5. **Logging**: Structured logging with correlation IDs
6. **Error Handling**: Retry logic with exponential backoff
7. **Sandboxing**: Additional isolation for test execution
8. **CI/CD**: Integrate with Jenkins, CircleCI, or GitHub Actions

### Deployment Options

```bash
# Docker
docker build -t api-testing-agent .
docker run -p 8000:8000 --env-file .env api-testing-agent

# Kubernetes
kubectl apply -f k8s/deployment.yaml

# Cloud Run / Lambda
# (See deployment guides in docs/)
```

---

## ğŸ› Troubleshooting

### Issue: "No module named 'requests'"
```bash
pip install -r requirements.txt
```

### Issue: "Failed to fetch OpenAPI spec"
- Check if the URL is accessible
- Verify it's a valid OpenAPI/Swagger spec
- Check for CORS or authentication requirements
- The system tries: `/openapi`, `/openapi.json`, `/openapi.yaml`

### Issue: "Authentication failed" with Jira
- Regenerate Jira API token
- Verify JIRA_USER is your email (not username)
- Check JIRA_BASE URL format

### Issue: Docker tests failing
- Ensure Docker daemon is running
- Check Docker has access to mount volumes
- Verify network connectivity

---

## ğŸ“Š Example Workflow

```
Developer creates Epic:
  "User Management API - http://localhost:8000/openapi.json"
          â†“
Jira sends webhook to our service
          â†“
Service fetches Epic details
          â†“
Extracts & fetches OpenAPI specification
          â†“
Sends Epic + OpenAPI spec to GPT-4
          â†“
GPT-4 generates comprehensive pytest tests
          â†“
Tests committed to: auto/tests/EPIC-123/20251023T120000Z
          â†“
Tests run in Docker container
          â†“
Results posted back to Jira Epic as comment
          â†“
Developer reviews tests and results in Jira
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

[Your License Here]

---

## ğŸ™ Acknowledgments

- OpenAI for GPT-4 API
- Atlassian for Jira API
- FastAPI framework
- pytest and Schemathesis communities

---

## ğŸ“ Support

For issues and questions:
- GitHub Issues: [Your repo URL]
- Documentation: [helper.md](helper.md)
- Email: [Your email]

---

**Built with â¤ï¸ using Python, FastAPI, and GPT-4**


