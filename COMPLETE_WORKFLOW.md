# Complete End-to-End Workflow ğŸ”„

## ğŸ“Š Mermaid Flow Diagrams

### Complete System Flow (Jira to Jenkins)

```mermaid
graph TB
    subgraph Input["ğŸ”¹ Input Sources"]
        Jira[Jira Epic<br/>OpenAPI URL + Requirements]
        Webhook[FastAPI Webhook<br/>/jira/webhook]
        Runner[Polling Runner<br/>time_sleep + prefix]
    end

    subgraph Config["âš™ï¸ Configuration"]
        YAML[config.yaml<br/>runner.prefix_jira_name<br/>openai.review_threshold<br/>models]
        ENV[.env secrets<br/>JIRA_TOKEN<br/>GITHUB_TOKEN<br/>OPENAI_API_KEY]
    end

    subgraph Orchestrator["ğŸ¯ Orchestration Pipeline"]
        Fetch[Fetch Epic<br/>fetch_issue]
        Parse[Parse Contract<br/>extract_contract]
        
        subgraph AI["ğŸ¤– AI Processing Chain"]
            AI1[AI-1: Eligibility Gate<br/>check_epic_eligibility<br/>should_proceed: bool]
            AI2[AI-2: Test Generator<br/>generate_tests<br/>pytest/RestAssured]
            AI3[AI-3: Review<br/>review_and_fix_tests<br/>score + notes]
            AI4[AI-4: Refine<br/>refine_tests_with_notes<br/>corrected files]
        end
        
        Decision{avg score<br/><= threshold?}
        SaveMeta[Save metadata.json<br/>gate+testcases+reviewer+refiner]
        SaveLocal[Save Locally<br/>output/EPIC_timestamp/]
    end

    subgraph Deployment["ğŸš€ Deployment & CI/CD"]
        GitHub[Push to GitHub<br/>auto/tests/EPIC/timestamp]
        Jenkins[Jenkins Pipeline<br/>pytest/mvn test<br/>JUnit results]
        Comment[Post to Jira<br/>test results + branch]
    end

    Jira -->|webhook event| Webhook
    Jira -.->|JQL poll| Runner
    Webhook -->|prefix filter| Fetch
    Runner -->|prefix filter| Fetch
    
    YAML -.->|config| Runner
    YAML -.->|thresholds| Decision
    ENV -.->|secrets| AI

    Fetch --> Parse
    Parse --> AI1
    
    AI1 -->|false| Comment
    AI1 -->|true| AI2
    AI2 --> AI3
    AI3 --> Decision
    
    Decision -->|yes| AI4
    Decision -->|no| SaveMeta
    AI4 --> SaveMeta
    
    SaveMeta --> SaveLocal
    SaveLocal -->|optional| GitHub
    GitHub --> Jenkins
    Jenkins --> Comment

    style AI1 fill:#e1f5ff
    style AI2 fill:#fff9e1
    style AI3 fill:#ffe1f5
    style AI4 fill:#e1ffe1
    style Decision fill:#ffd700
```

---

## ğŸ”„ Detailed Workflow

### Phase 1: Epic Creation & Configuration

```
Developer creates Jira Epic
       â”‚
       â”œâ”€ Summary: "QA Validation for Orders Service API"
       â”‚
       â””â”€ Description:
          â”œâ”€ OpenAPI URL: http://localhost:8002/openapi.json
          â”œâ”€ Requirements
          â””â”€ Test scenarios
```

### Phase 2: Test Generation (with eligibility gate)

```bash
# Direct single-epic run (new CLI)
python3 tests/test_orchestrator_flow.py --jira_name KAN-4 --language python --jenkins yes

# Or start the polling runner (process epics by project prefix)
python3 tests/test_orchestrator_flow.py --runner on --language python --jenkins no
```

**What Happens:**

```
Step 1: Fetch Epic from Jira
   â†“
   â”œâ”€ Connect to Jira API
   â”œâ”€ Fetch Epic details (KAN-4)
   â””â”€ Extract summary & description
   
Step 2: Extract OpenAPI Specification
   â†“
   â”œâ”€ Parse Epic description for OpenAPI URL
   â”œâ”€ Fetch OpenAPI spec (http://localhost:8002/openapi.json)
   â”œâ”€ Parse JSON/YAML format
   â””â”€ Extract 4 endpoints
   
Eligibility Gate (LLM)
   â†“
   â”œâ”€ Validate presence of OpenAPI URL or concrete endpoints
   â”œâ”€ Optionally verify accessibility signals
   â””â”€ Decide: {"should_proceed": true|false, "reason": "..."}
      â””â”€ If false â†’ skip pipeline and post comment to Jira
   
Step 3: Generate Tests with LLM (only if should_proceed=true)
   â†“
   â”œâ”€ Build prompt with Epic + OpenAPI spec
   â”œâ”€ Call OpenAI model (configurable)
   â”œâ”€ Parse response
   â””â”€ Generate 4-5 test files:
      â”œâ”€ tests/conftest.py (fixtures)
      â”œâ”€ tests/test_orders.py (main tests)
      â”œâ”€ tests/test_schemathesis.py (property tests)
      â”œâ”€ tests/requirements.txt (dependencies)
      â””â”€ tests/README.md (documentation)
   
Step 4: Save Tests Locally
   â†“
   â””â”€ output/KAN-4_20251024_124753/
      â”œâ”€ tests/
      â””â”€ GENERATION_INFO.txt
   
Step 5: Push to GitHub
   â†“
   â”œâ”€ Create branch: auto/tests/KAN-4/20251024T071757Z
   â”œâ”€ Commit all test files
   â”œâ”€ Push to GitHub
   â””â”€ Post comment to Jira Epic
```

### Phase 3: Jenkins Automation

```
GitHub receives push
   â†“
Webhook triggers Jenkins
   â†“
Jenkins detects new branch matching "auto/tests/*"
   â†“
   â”œâ”€ Clone repository
   â”œâ”€ Read Jenkinsfile
   â””â”€ Start pipeline
   
Pipeline Execution:
   â†“
   â”œâ”€ Stage 1: Checkout
   â”‚  â””â”€ Pull code from branch
   â”‚
   â”œâ”€ Stage 2: Detect Test Type
   â”‚  â”œâ”€ Check for pom.xml (Java)
   â”‚  â””â”€ Check for tests/requirements.txt (Python)
   â”‚
   â”œâ”€ Stage 3: Setup Environment
   â”‚  â”œâ”€ Python: Create venv, install deps
   â”‚  â””â”€ Java: Verify Maven, JDK
   â”‚
   â”œâ”€ Stage 4: Run Tests
   â”‚  â”œâ”€ Python: pytest -v --junitxml --html
   â”‚  â””â”€ Java: mvn clean test
   â”‚
   â”œâ”€ Stage 5: Publish Results
   â”‚  â”œâ”€ Parse JUnit XML
   â”‚  â”œâ”€ Generate HTML report
   â”‚  â””â”€ Archive artifacts
   â”‚
   â””â”€ Stage 6: Post Results
      â”œâ”€ Update Jira Epic with results
      â”œâ”€ Send notifications (email/Slack)
      â””â”€ Mark build as SUCCESS/FAILURE
```

### Phase 4: Results & Reporting

```
Test Results Published:
   â†“
   â”œâ”€ Jenkins UI
   â”‚  â”œâ”€ Test Results tab
   â”‚  â”œâ”€ HTML Report
   â”‚  â””â”€ Console Output
   â”‚
   â”œâ”€ Jira Epic Comment
   â”‚  â”œâ”€ Test summary
   â”‚  â”œâ”€ Pass/Fail counts
   â”‚  â””â”€ Build URL
   â”‚
   â””â”€ Notifications
      â”œâ”€ Email to team
      â””â”€ Slack message
```

## ğŸš€ Quick Commands (updated)

```bash
# Direct single-epic run (python)
python3 tests/test_orchestrator_flow.py --jira_name KAN-4 --language python --jenkins yes

# Direct single-epic run (java)
python3 tests/test_orchestrator_flow.py --jira_name KAN-4 --language java --jenkins yes

# Start polling runner (no GitHub push)
python3 tests/test_orchestrator_flow.py --runner on --language python --jenkins no

# Start service + runner
python3 main.py --runner on --language python --jenkins no --port 8002

# Test configuration and integrations
python3 tests/test_jira_integration.py
python3 tests/test_github_config.py
python3 setup_jenkins_integration.py
```

## ğŸ“š Documentation

- **Quick Start**: `QUICK_START.md`
- **GitHub Setup**: `GITHUB_SETUP.md`
- **Jenkins Setup**: `JENKINS_SETUP.md`
- **Jenkins Quick Start**: `JENKINS_QUICK_START.md`
- **Full Documentation**: `README.md`
- **Architecture**: `helper.md`
- **Implementation**: `IMPLEMENTATION_SUMMARY.md`

## ğŸ‰ Success Metrics

Your system is working when:
- âœ… Tests generate in < 30 seconds
- âœ… GitHub push successful
- âœ… Jenkins build starts automatically
- âœ… Test results appear in Jenkins
- âœ… Jira comment posted with results

---

**Congratulations! Your API Testing Agent is fully operational! ğŸš€**

